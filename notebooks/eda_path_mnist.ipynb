{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccdf9df0",
   "metadata": {},
   "source": [
    "## Exploratory data analysis for the PathMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa24e1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if root not in sys.path:\n",
    "    sys.path.insert(0, root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "391cc5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02566e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████████████████████████████▊     | 3.68G/4.26G [16:25<02:34, 3.73MB/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\n                Automatic download failed! Please download pathmnist_128.npz manually.\n                1. [Optional] Check your network connection: \n                    Go to https://github.com/MedMNIST/MedMNIST/ and find the Zenodo repository\n                2. Download the npz file from the Zenodo repository or its Zenodo data link: \n                    https://zenodo.org/records/10519652/files/pathmnist_128.npz?download=1\n                3. [Optional] Verify the MD5: \n                    ac42d08fb904d92c244187169d1fd1d9\n                4. Put the npz file under your MedMNIST root folder: \n                    ./data/dataset/pathmnist\n                ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/IMAnotebook/lib/python3.12/site-packages/medmnist/dataset.py:106\u001b[0m, in \u001b[0;36mMedMNIST.download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download_url\n\u001b[0;32m--> 106\u001b[0m     download_url(\n\u001b[1;32m    107\u001b[0m         url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize_flag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    108\u001b[0m         root\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot,\n\u001b[1;32m    109\u001b[0m         filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize_flag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    110\u001b[0m         md5\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMD5\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize_flag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    111\u001b[0m     )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/IMAnotebook/lib/python3.12/site-packages/torchvision/datasets/utils.py:140\u001b[0m, in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_integrity(fpath, md5):\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found or corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: File not found or corrupted.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     get_data, visualize_batch, visualize_one_sample_per_class, visualize_one_sample_per_class_single_row,\n\u001b[1;32m      5\u001b[0m     get_class_distribution, plot_class_distribution, compute_mean_std)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Loads PathMNIST loaders (adapted to 128x128)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m train_loader, val_loader, test_loader \u001b[38;5;241m=\u001b[39m get_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPathMNIST\u001b[39m\u001b[38;5;124m\"\u001b[39m, im_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmedmnist\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m INFO\n",
      "File \u001b[0;32m~/Documents/telecom/gitlab/2025-im06-explainability/data/mnist_loader.py:46\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(dataset_name, im_size, batch_size, data_dir)\u001b[0m\n\u001b[1;32m     43\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(root_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# separate in training, validation and test\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m DatasetClass(root\u001b[38;5;241m=\u001b[39mroot_dir, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, transform\u001b[38;5;241m=\u001b[39mtransform, size \u001b[38;5;241m=\u001b[39m im_size, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m val_ds \u001b[38;5;241m=\u001b[39m DatasetClass(root\u001b[38;5;241m=\u001b[39mroot_dir, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m,   transform\u001b[38;5;241m=\u001b[39mtransform, size \u001b[38;5;241m=\u001b[39m im_size, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     48\u001b[0m test_ds \u001b[38;5;241m=\u001b[39m DatasetClass(root\u001b[38;5;241m=\u001b[39mroot_dir, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m,  transform\u001b[38;5;241m=\u001b[39mtransform, size \u001b[38;5;241m=\u001b[39m im_size, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/IMAnotebook/lib/python3.12/site-packages/medmnist/dataset.py:56\u001b[0m, in \u001b[0;36mMedMNIST.__init__\u001b[0;34m(self, split, transform, target_transform, download, as_rgb, root, size, mmap_mode)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to setup the default `root` directory. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease specify and create the `root` directory manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     53\u001b[0m     )\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\n\u001b[1;32m     59\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize_flag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m ):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m You can set `download=True` to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/IMAnotebook/lib/python3.12/site-packages/medmnist/dataset.py:113\u001b[0m, in \u001b[0;36mMedMNIST.download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m     download_url(\n\u001b[1;32m    107\u001b[0m         url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize_flag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    108\u001b[0m         root\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot,\n\u001b[1;32m    109\u001b[0m         filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize_flag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    110\u001b[0m         md5\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMD5\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize_flag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    111\u001b[0m     )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124m        Automatic download failed! Please download \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize_flag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npz manually.\u001b[39m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124m        1. [Optional] Check your network connection: \u001b[39m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124m            Go to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHOMEPAGE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and find the Zenodo repository\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124m        2. Download the npz file from the Zenodo repository or its Zenodo data link: \u001b[39m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize_flag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124m        3. [Optional] Verify the MD5: \u001b[39m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMD5\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize_flag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124m        4. Put the npz file under your MedMNIST root folder: \u001b[39m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    125\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \n                Automatic download failed! Please download pathmnist_128.npz manually.\n                1. [Optional] Check your network connection: \n                    Go to https://github.com/MedMNIST/MedMNIST/ and find the Zenodo repository\n                2. Download the npz file from the Zenodo repository or its Zenodo data link: \n                    https://zenodo.org/records/10519652/files/pathmnist_128.npz?download=1\n                3. [Optional] Verify the MD5: \n                    ac42d08fb904d92c244187169d1fd1d9\n                4. Put the npz file under your MedMNIST root folder: \n                    ./data/dataset/pathmnist\n                "
     ]
    }
   ],
   "source": [
    "# Load functions from the data folder\n",
    "\n",
    "from data import (\n",
    "    get_data, visualize_batch, visualize_one_sample_per_class, visualize_one_sample_per_class_single_row,\n",
    "    get_class_distribution, plot_class_distribution, compute_mean_std)\n",
    "\n",
    "\n",
    "# Loads PathMNIST loaders (adapted to 128x128)\n",
    "train_loader, val_loader, test_loader = get_data(\"PathMNIST\", im_size=128)\n",
    "\n",
    "from medmnist import INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09991618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import (AlexNet128, DeconvNet128)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Initial system configurations\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "dataset_dir = \"../data/dataset\"\n",
    "models_dir = \"../models/trained_models/\"\n",
    "\n",
    "dataset_name = \"PathMNIST\"\n",
    "num_classes = 9\n",
    "model_file = \"alexnet128_pathmnist.pth\"\n",
    "\n",
    "alexnet = AlexNet128(num_classes=num_classes)\n",
    "state_dict = torch.load(models_dir+model_file, map_location=torch.device(device) )\n",
    "alexnet.load_state_dict(state_dict)\n",
    "alexnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8595ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "deconvnet = DeconvNet128(alexnet)\n",
    "deconvnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4add025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking class distribution\n",
    "\n",
    "info = INFO[\"pathmnist\"]\n",
    "class_names = {int(k): v for k, v in info[\"label\"].items()}  # converted keys to int\n",
    "\n",
    "dist = get_class_distribution(train_loader)\n",
    "print(\"Frequency per class:\", dict(dist))\n",
    "plot_class_distribution(dist, class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31d70b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See a sample from each class\n",
    "visualize_one_sample_per_class(train_loader, class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481d9f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single row with one sample from each class\n",
    "visualize_one_sample_per_class_single_row(train_loader, class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23870cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single row with one sample from each class\n",
    "visualize_one_sample_per_class_single_row(train_loader, class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cff9629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single row with one sample from each class\n",
    "visualize_one_sample_per_class_single_row(train_loader, class_names=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18822865",
   "metadata": {},
   "source": [
    "### Occlusion tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0cd90d",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0623f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_occlusion_result(img, label, x, y, patch_size):\n",
    "    \"\"\"\n",
    "    Applies grey square occlusion and returns relevant values.\n",
    "    \"\"\"\n",
    "    occluded = img.clone()                                # copy input image\n",
    "    occluded[:, :, y:y+patch_size, x:x+patch_size] = 0.5  # apply the grey square\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits_occ, acts_occ = alexnet(occluded)                  # forward pass the occluded img through the model\n",
    "        feat5_occ = acts_occ[\"feat5\"]                             # extract feature map activations from layer 5\n",
    "        prob_occ = F.softmax(logits_occ, dim=1)[0, label].item()  # compute probability of true class with softmax\n",
    "        pred_class = logits_occ.argmax(dim=1).item()              # get the predicted class\n",
    "        # compute the total activation of the strongest feature map in layer 5\n",
    "        activation = feat5_occ[0, strongest_idx].sum().item()    \n",
    "\n",
    "    # Return:\n",
    "    # the occluded image (converted to shape [3, H, W] and moved to CPU)\n",
    "    # the total activation value of the strongest feature\n",
    "    # the probability of the true class\n",
    "    # the predicted class index\n",
    "    return occluded.squeeze().cpu(), activation, prob_occ, pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ded7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_occlusion_maps(img, label, patch_size=64, stride=10):\n",
    "    H, W = img.shape[2:]\n",
    "    xs = list(range(0, W - patch_size + 1, stride))\n",
    "    ys = list(range(0, H - patch_size + 1, stride))\n",
    "\n",
    "    activation_map = np.zeros((len(ys), len(xs)))\n",
    "    prob_map = np.zeros((len(ys), len(xs)))\n",
    "    pred_map = np.zeros((len(ys), len(xs)), dtype=int)\n",
    "\n",
    "    for i, y in enumerate(ys):\n",
    "        for j, x in enumerate(xs):\n",
    "            _, act, prob, pred = get_occlusion_result(img, label, x, y, patch_size)\n",
    "            activation_map[i, j] = act\n",
    "            prob_map[i, j] = prob\n",
    "            pred_map[i, j] = pred\n",
    "\n",
    "    return activation_map, prob_map, pred_map,xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178eb68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_occlusion_entry(occluded_img, activation_map, prob_map, pred_map, pred_class, recon_np, x, y, xs, ys):\n",
    "\n",
    "    fig, axs = plt.subplots(1, 5, figsize=(20, 4))\n",
    "\n",
    "    axs[0].imshow(to_pil_image(occluded_img))\n",
    "    axs[0].set_title(f\"(a) Patch at ({x},{y})\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    # im1 = axs[1].imshow(activation_map, cmap=\"hot\", extent=[xs[0], xs[-1]+1, ys[-1]+1, ys[0]])\n",
    "    axs[1].imshow(activation_map, cmap=\"hot\", extent=[xs[0], xs[-1]+1, ys[-1]+1, ys[0]])\n",
    "    axs[1].set_title(\"(b) Layer 5 activation map\")\n",
    "    # fig.colorbar(im1, ax=axs[1])\n",
    "\n",
    "    axs[2].imshow(recon_np)\n",
    "    axs[2].set_title(f\"(c) Feature projection\")\n",
    "    axs[2].axis(\"off\")\n",
    "\n",
    "\n",
    "    im2 = axs[3].imshow(prob_map, cmap=\"coolwarm\", vmin=0, vmax=1, extent=[xs[0], xs[-1]+1, ys[-1]+1, ys[0]])\n",
    "    axs[3].set_title(f\"(d) P(true class)\")\n",
    "    fig.colorbar(im2, ax=axs[3])\n",
    "    \n",
    "\n",
    "    pred_name = class_names.get(pred_class, str(pred_class))\n",
    "    axs[4].imshow(pred_map, cmap=\"tab20\", extent=[xs[0], xs[-1]+1, ys[-1]+1, ys[0]])\n",
    "    axs[4].set_title(f\"(e) Predicted class:\\n{pred_name}\")\n",
    "    axs[4].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08244ecb",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209eddbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random image from a random batch in the test set\n",
    "batch = next(iter(test_loader))\n",
    "rand_idx = random.randint(0, len(batch[0]) - 1)\n",
    "img, label = batch[0][rand_idx], batch[1][rand_idx]\n",
    "img = img.unsqueeze(0).to(device)\n",
    "label = label.item()\n",
    "\n",
    "print(\"Random image selected.\")\n",
    "print(\"True label index:\", label)\n",
    "print(\"True label name:\", class_names[label])\n",
    "\n",
    "# Forward pass through model to get original activations\n",
    "alexnet.eval()\n",
    "with torch.no_grad():\n",
    "    logits, acts = alexnet(img)\n",
    "\n",
    "# Get strongest feature map in layer 5\n",
    "feat5 = acts[\"feat5\"]\n",
    "strongest_idx = feat5[0].sum(dim=(1, 2)).argmax().item()\n",
    "\n",
    "# Prepare one-hot map to project back the strongest feature\n",
    "# one_hot = torch.zeros_like(feat5)\n",
    "# one_hot[0, strongest_idx] = feat5[0, strongest_idx]\n",
    "\n",
    "feat_pos = torch.clamp(feat5, min=0.0)\n",
    "\n",
    "# Get the projected feature visualization using deconvnet\n",
    "with torch.no_grad():\n",
    "    # recon = deconvnet(one_hot, acts, layer=5)\n",
    "    recon    = deconvnet(feat_pos, acts, layer=5)\n",
    "recon_np = recon.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "recon_np = (recon_np - recon_np.min()) / (recon_np.max() - recon_np.min())\n",
    "\n",
    "\n",
    "# Generate heatmaps\n",
    "activation_map, prob_map, pred_map, xs, ys = generate_occlusion_maps(img, label, patch_size=32, stride=20)\n",
    "\n",
    "\n",
    "# Use occlusion \n",
    "x, y = xs[len(xs)//2], ys[len(ys)//2]\n",
    "occ_img, _, _, pred = get_occlusion_result(img, label, x, y, patch_size=110)\n",
    "\n",
    "# Plot\n",
    "plot_occlusion_entry(occ_img, activation_map, prob_map, pred_map, pred, recon_np, x, y, xs, ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b71d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random image from a random batch in the test set\n",
    "batch = next(iter(test_loader))\n",
    "rand_idx = random.randint(0, len(batch[0]) - 1)\n",
    "img, label = batch[0][rand_idx], batch[1][rand_idx]\n",
    "img = img.unsqueeze(0).to(device)\n",
    "label = label.item()\n",
    "\n",
    "print(\"Random image selected.\")\n",
    "print(\"True label index:\", label)\n",
    "print(\"True label name:\", class_names[label])\n",
    "\n",
    "# Forward pass through model to get original activations\n",
    "alexnet.eval()\n",
    "with torch.no_grad():\n",
    "    logits, acts = alexnet(img)\n",
    "\n",
    "# Get strongest feature map in layer 5\n",
    "feat5 = acts[\"feat5\"]\n",
    "strongest_idx = feat5[0].sum(dim=(1, 2)).argmax().item()\n",
    "\n",
    "# Prepare one-hot map to project back the strongest feature\n",
    "one_hot = torch.zeros_like(feat5)\n",
    "one_hot[0, strongest_idx] = feat5[0, strongest_idx]\n",
    "\n",
    "# Get the projected feature visualization using deconvnet\n",
    "with torch.no_grad():\n",
    "    recon = deconvnet(one_hot, acts, layer=5)\n",
    "recon_np = recon.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "recon_np = (recon_np - recon_np.min()) / (recon_np.max() - recon_np.min())\n",
    "\n",
    "\n",
    "# Generate heatmaps\n",
    "activation_map, prob_map, pred_map, xs, ys = generate_occlusion_maps(img, label, patch_size=64, stride=20)\n",
    "\n",
    "\n",
    "# Use occlusion \n",
    "x, y = xs[len(xs)//2], ys[len(ys)//2]\n",
    "patch_size=110\n",
    "center_x = (128 - patch_size) // 2\n",
    "center_y = (128 - patch_size) // 2\n",
    "occ_img, _, _, pred = get_occlusion_result(img, label, center_x, center_y, patch_size=patch_size)\n",
    "\n",
    "\n",
    "# Plot\n",
    "plot_occlusion_entry(occ_img, activation_map, prob_map, pred_map, pred, recon_np, x, y, xs, ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292fd914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random image from a random batch in the test set\n",
    "batch = next(iter(test_loader))\n",
    "rand_idx = random.randint(0, len(batch[0]) - 1)\n",
    "img, label = batch[0][rand_idx], batch[1][rand_idx]\n",
    "img = img.unsqueeze(0).to(device)\n",
    "label = label.item()\n",
    "\n",
    "print(\"Random image selected.\")\n",
    "print(\"True label index:\", label)\n",
    "print(\"True label name:\", class_names[label])\n",
    "\n",
    "# Forward pass through model to get original activations\n",
    "alexnet.eval()\n",
    "with torch.no_grad():\n",
    "    logits, acts = alexnet(img)\n",
    "\n",
    "# Get strongest feature map in layer 5\n",
    "feat5 = acts[\"feat5\"]\n",
    "strongest_idx = feat5[0].sum(dim=(1, 2)).argmax().item()\n",
    "\n",
    "# Prepare one-hot map to project back the strongest feature\n",
    "one_hot = torch.zeros_like(feat5)\n",
    "one_hot[0, strongest_idx] = feat5[0, strongest_idx]\n",
    "\n",
    "# Get the projected feature visualization using deconvnet\n",
    "with torch.no_grad():\n",
    "    recon = deconvnet(one_hot, acts, layer=5)\n",
    "recon_np = recon.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "recon_np = (recon_np - recon_np.min()) / (recon_np.max() - recon_np.min())\n",
    "\n",
    "\n",
    "# Generate heatmaps\n",
    "activation_map, prob_map, pred_map, xs, ys = generate_occlusion_maps(img, label, patch_size=64, stride=20)\n",
    "\n",
    "\n",
    "# Use occlusion \n",
    "x, y = xs[len(xs)//2], ys[len(ys)//2]\n",
    "patch_size=110\n",
    "center_x = (128 - patch_size) // 2\n",
    "center_y = (128 - patch_size) // 2\n",
    "occ_img, _, _, pred = get_occlusion_result(img, label, center_x, center_y, patch_size=patch_size)\n",
    "\n",
    "\n",
    "# Plot\n",
    "plot_occlusion_entry(occ_img, activation_map, prob_map, pred_map, pred, recon_np, x, y, xs, ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74b10b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random image from a random batch in the test set\n",
    "batch = next(iter(test_loader))\n",
    "rand_idx = random.randint(0, len(batch[0]) - 1)\n",
    "img, label = batch[0][rand_idx], batch[1][rand_idx]\n",
    "img = img.unsqueeze(0).to(device)\n",
    "label = label.item()\n",
    "\n",
    "print(\"Random image selected.\")\n",
    "print(\"True label index:\", label)\n",
    "print(\"True label name:\", class_names[label])\n",
    "\n",
    "# Forward pass through model to get original activations\n",
    "alexnet.eval()\n",
    "with torch.no_grad():\n",
    "    logits, acts = alexnet(img)\n",
    "\n",
    "# Get strongest feature map in layer 5\n",
    "feat5 = acts[\"feat5\"]\n",
    "strongest_idx = feat5[0].sum(dim=(1, 2)).argmax().item()\n",
    "\n",
    "# Prepare one-hot map to project back the strongest feature\n",
    "one_hot = torch.zeros_like(feat5)\n",
    "one_hot[0, strongest_idx] = feat5[0, strongest_idx]\n",
    "\n",
    "# Get the projected feature visualization using deconvnet\n",
    "with torch.no_grad():\n",
    "    recon = deconvnet(one_hot, acts, layer=5)\n",
    "recon_np = recon.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "recon_np = (recon_np - recon_np.min()) / (recon_np.max() - recon_np.min())\n",
    "\n",
    "\n",
    "# Generate heatmaps\n",
    "activation_map, prob_map, pred_map, xs, ys = generate_occlusion_maps(img, label, patch_size=64, stride=20)\n",
    "\n",
    "\n",
    "# Use occlusion \n",
    "x, y = xs[len(xs)//2], ys[len(ys)//2]\n",
    "occ_img, _, _, pred = get_occlusion_result(img, label, x, y, patch_size=80)\n",
    "\n",
    "# Plot\n",
    "plot_occlusion_entry(occ_img, activation_map, prob_map, pred_map, pred, recon_np, x, y, xs, ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101e8753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random image from a random batch in the test set\n",
    "batch = next(iter(test_loader))\n",
    "rand_idx = random.randint(0, len(batch[0]) - 1)\n",
    "img, label = batch[0][rand_idx], batch[1][rand_idx]\n",
    "img = img.unsqueeze(0).to(device)\n",
    "label = label.item()\n",
    "\n",
    "print(\"Random image selected.\")\n",
    "print(\"True label index:\", label)\n",
    "print(\"True label name:\", class_names[label])\n",
    "\n",
    "# Forward pass through model to get original activations\n",
    "alexnet.eval()\n",
    "with torch.no_grad():\n",
    "    logits, acts = alexnet(img)\n",
    "\n",
    "# Get strongest feature map in layer 5\n",
    "feat5 = acts[\"feat5\"]\n",
    "strongest_idx = feat5[0].sum(dim=(1, 2)).argmax().item()\n",
    "\n",
    "# Prepare one-hot map to project back the strongest feature\n",
    "one_hot = torch.zeros_like(feat5)\n",
    "one_hot[0, strongest_idx] = feat5[0, strongest_idx]\n",
    "\n",
    "# Get the projected feature visualization using deconvnet\n",
    "with torch.no_grad():\n",
    "    recon = deconvnet(one_hot, acts, layer=5)\n",
    "recon_np = recon.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "recon_np = (recon_np - recon_np.min()) / (recon_np.max() - recon_np.min())\n",
    "\n",
    "\n",
    "# Generate heatmaps\n",
    "activation_map, prob_map, pred_map, xs, ys = generate_occlusion_maps(img, label, patch_size=64, stride=10)\n",
    "\n",
    "\n",
    "# Use occlusion \n",
    "x, y = xs[len(xs)//2], ys[len(ys)//2]\n",
    "occ_img, _, _, pred = get_occlusion_result(img, label, x, y, patch_size=64)\n",
    "\n",
    "# Plot\n",
    "plot_occlusion_entry(occ_img, activation_map, prob_map, pred_map, pred, recon_np, x, y, xs, ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d23b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random image from a random batch in the test set\n",
    "batch = next(iter(test_loader))\n",
    "rand_idx = random.randint(0, len(batch[0]) - 1)\n",
    "img, label = batch[0][rand_idx], batch[1][rand_idx]\n",
    "img = img.unsqueeze(0).to(device)\n",
    "label = label.item()\n",
    "\n",
    "print(\"Random image selected.\")\n",
    "print(\"True label index:\", label)\n",
    "print(\"True label name:\", class_names[label])\n",
    "\n",
    "# Forward pass through model to get original activations\n",
    "alexnet.eval()\n",
    "with torch.no_grad():\n",
    "    logits, acts = alexnet(img)\n",
    "\n",
    "# Get strongest feature map in layer 5\n",
    "feat5 = acts[\"feat5\"]\n",
    "strongest_idx = feat5[0].sum(dim=(1, 2)).argmax().item()\n",
    "\n",
    "# Prepare one-hot map to project back the strongest feature\n",
    "one_hot = torch.zeros_like(feat5)\n",
    "one_hot[0, strongest_idx] = feat5[0, strongest_idx]\n",
    "\n",
    "# Get the projected feature visualization using deconvnet\n",
    "with torch.no_grad():\n",
    "    recon = deconvnet(one_hot, acts, layer=5)\n",
    "recon_np = recon.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "recon_np = (recon_np - recon_np.min()) / (recon_np.max() - recon_np.min())\n",
    "\n",
    "\n",
    "# Generate heatmaps\n",
    "activation_map, prob_map, pred_map, xs, ys = generate_occlusion_maps(img, label, patch_size=32, stride=10)\n",
    "\n",
    "\n",
    "# Use occlusion \n",
    "x, y = xs[len(xs)//2], ys[len(ys)//2]\n",
    "occ_img, _, _, pred = get_occlusion_result(img, label, x, y, patch_size=32)\n",
    "\n",
    "# Plot\n",
    "plot_occlusion_entry(occ_img, activation_map, prob_map, pred_map, pred, recon_np, x, y, xs, ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b1eacb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
